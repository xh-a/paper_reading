## 为什么attention要除以d
Attention = softmax($\frac{QK^T}{\sqrt{d}}$)
- d越大，$q \cdot k^T$ 方差越大
- 方差变大，意味着元素之间差值变大
- $q \cdot k^T$方差越大，导致softmax退化成argmax
- argmax很多0，会导致梯度消失

## ReLU的优缺点
优点：
- 计算容易，比exp快
- 缓解梯度消失问题，相比于sigmoid和tanh
- 非正数都设置为0， 起到稀疏化的作用，起到正则化的作用

缺点：
- 死亡神经元
- 范围[0, inf]， 导致网络崩溃
- 0处不可微
- not zero-centered, 可能会导致反向传播时zig-zagging的问题

## 解释一下稀疏性
稀疏性=方差？
稀疏性=0更多、更加解耦？
稀疏性的本质通过朴素贝叶斯可以退出来为正则项的指数息息相关。
稀疏性越大，则正则化所需要的指数越小。
部分理解： dropout 相当于增强数据稀疏性。
噪声导致过拟合，通过添加正则化手段降低过拟合，正则化有两个参数$\lambda$和正则指数。根据数据噪声大小以及数据的稀疏性来分别控制。 如果知道数据和噪声的分布，这个值可以通过贝叶斯公式推出来。

## 码率减少的根本原因
引入了更多的先验

- 模型结构是一种先验
- 各种各样的引导loss是一种先验
- 不同的训练策略是一种先验
- 顺序、scale、分组 都是不同程度的先验表达

